{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/randiepathirage/4th/blob/main/implementation/Prev_Implementation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHvUwMhr7umk"
      },
      "outputs": [],
      "source": [
        "# data imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sns.barplot(x='frequency',y='trigrams',data=trigrams.head(30))\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from unicodedata import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM,Bidirectional,Input,Concatenate\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7PDAoz4c8pXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b480681-c524-48b8-ce30-901d910c3c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Setup"
      ],
      "metadata": {
        "id": "z9q5cUmGph8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"e5_1\"\n",
        "batch_size = 32 #Batch size for training.\n",
        "epochs = 50 #Number of epochs to train for.\n",
        "latent_dim = 256 #Latent dimensionality of the encoding space.\n",
        "num_samples = 24415 #Number of samples to train on,\n",
        "test_count = round(num_samples*0.2)"
      ],
      "metadata": {
        "id": "rHuAMgM3pg2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Datasets"
      ],
      "metadata": {
        "id": "5Vi05EEXq5JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/4th year Research/data collection/data/ALL_Unicode.csv\"\n",
        "df = pd.read_csv(path,names=[\"Singlish\",\"Sinhala\"],dtype=str).astype(str)\n",
        "\n",
        "input_data = pd.DataFrame(df.Singlish.values[:num_samples])\n",
        "input_data.columns = ['Singlish']\n",
        "\n",
        "target_data = pd.DataFrame(df.Sinhala.values[:num_samples])\n",
        "target_data.columns = ['Sinhala']"
      ],
      "metadata": {
        "id": "JD0xM1hQ8Hm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JBg0IiQHyYWV",
        "outputId": "40583b84-eddb-4675-de3d-c6b4645e2d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Singlish\n",
              "0                                               \\nOw ane\n",
              "1                                                  Ne ne\n",
              "2                                                \\t Hari\n",
              "3                                   5.30ta nam yanne naa\n",
              "4                      6 shirts kohomath 7000/= ne wenne\n",
              "...                                                  ...\n",
              "24410             heta nm admission ganna yanna wei neda\n",
              "24411  Heta ofc yanawa akila, Man gihin call ekak dennan\n",
              "24412                 Heta off wena welaw mg ekk dl ynna\n",
              "24413          Heta office ekata gihin iwara karagannawa\n",
              "24414                        Heta office yanna thiyamawa\n",
              "\n",
              "[24415 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00710bde-a740-4cd3-81fd-28668a33f0b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Singlish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nOw ane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ne ne</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\t Hari</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.30ta nam yanne naa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6 shirts kohomath 7000/= ne wenne</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24410</th>\n",
              "      <td>heta nm admission ganna yanna wei neda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24411</th>\n",
              "      <td>Heta ofc yanawa akila, Man gihin call ekak dennan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24412</th>\n",
              "      <td>Heta off wena welaw mg ekk dl ynna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24413</th>\n",
              "      <td>Heta office ekata gihin iwara karagannawa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24414</th>\n",
              "      <td>Heta office yanna thiyamawa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24415 rows Ã— 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00710bde-a740-4cd3-81fd-28668a33f0b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00710bde-a740-4cd3-81fd-28668a33f0b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00710bde-a740-4cd3-81fd-28668a33f0b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pre-processing fuctions"
      ],
      "metadata": {
        "id": "qxRsERF2tu5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text (text):\n",
        "  text = normalize( 'NFD', text).encode( 'utf-8')\n",
        "  return text.decode('UTF-8')"
      ],
      "metadata": {
        "id": "WHMHO3DmjsJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_normalize(text):\n",
        "  text = normalize_text (text.lower())\n",
        "  return text"
      ],
      "metadata": {
        "id": "L6-FSnWdjvH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeUnnecessaryRepeats(word):\n",
        "  prev = word[0]\n",
        "  repeatCount =1\n",
        "  ans = word[0]\n",
        "  for i in range(1, len(word)):\n",
        "    if(word[i]==prev):\n",
        "      repeatCount+=1\n",
        "    else:\n",
        "      repeatCount=1\n",
        "\n",
        "    if(repeatCount<3):\n",
        "      ans+=word[i]\n",
        "      prev=word[i]\n",
        "  return ans"
      ],
      "metadata": {
        "id": "0J4DX9HPj1Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessSinglish(singText):\n",
        "  vowels = ['a', 'e', 'i', 'o', 'u']\n",
        "  specialConsonentSuffix = ['h']\n",
        "  singWords = singText.split()\n",
        "  finList = []\n",
        "  for singWord in singWords:\n",
        "    singWord = removeUnnecessaryRepeats(singWord)\n",
        "    buf =\"\"\n",
        "    for i in range(len(singWord)):\n",
        "      char = singWord[i]\n",
        "\n",
        "      if(char in vowels):\n",
        "        if(i<len(singWord)-1):\n",
        "          if(char == singWord[i+1]):\n",
        "            buf+=char\n",
        "          else:\n",
        "            buf+=char\n",
        "            finList.append(buf)\n",
        "            buf=\"\"\n",
        "        else:\n",
        "            buf+=char\n",
        "            finList.append(buf)\n",
        "            buf=\"\"\n",
        "      else:\n",
        "        if(i<len(singWord)-1):\n",
        "          if(char == singWord[i+1] or (singWord[i+1] not in vowels and singWord[i+1] not in specialConsonentSuffix)):\n",
        "            buf+=char\n",
        "            finList.append(buf)\n",
        "            buf=\"\"\n",
        "          else:\n",
        "            buf+=char\n",
        "        else:\n",
        "          buf+=char\n",
        "          finList.append(buf)\n",
        "          buf=\"\"\n",
        "    finList.append(\"$\")\n",
        "  return \" \".join(finList)"
      ],
      "metadata": {
        "id": "DP44vVhgkC6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessSinhala(sinText):\n",
        "  sinWords = sinText.split()\n",
        "  finList =[]\n",
        "  for sinWord in sinWords:\n",
        "    charList = re.findall(r'\\w\\W?', sinWord.strip())\n",
        "    finList.extend(charList)\n",
        "    finList.append(\"$\")\n",
        "  return \"\".join(finList)"
      ],
      "metadata": {
        "id": "hLJKKjovtkX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**#pre-processing data\n"
      ],
      "metadata": {
        "id": "dENR3ud-6m4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data['Singlish'] = input_data['Singlish'].apply(lambda x: preProcessSinglish(clean_and_normalize(x)))\n",
        "target_data['Sinhala'] = target_data['Sinhala'].apply(lambda x:preProcessSinhala(clean_and_normalize(x))).apply(lambda x: '<S> '+x+' <E>')\n",
        "#test_input_data['singlish'] = test_input_data['singlish'].apply(lambda x: preProcessSinglish(clean_and_normalize(x)))"
      ],
      "metadata": {
        "id": "XISoiVRr6sAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*#Token Generation "
      ],
      "metadata": {
        "id": "_tf8xN7M8O9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "test_input_texts = []\n",
        "input_TUs = set()\n",
        "target_TUs = set()\n",
        "\n",
        "input_texts = input_data.Singlish\n",
        "target_texts = target_data.Sinhala\n",
        "#test_input_texts = test_input_data.singlish\n",
        "#test_target_texts = test_target_data.sinhala\n",
        "\n",
        "for input_text in input_texts:\n",
        "  for TU in input_text.split():\n",
        "    if TU not in input_TUs:\n",
        "      input_TUs.add(TU)\n",
        "\n",
        "for target_text in target_texts:\n",
        "  for TU in target_text.split():\n",
        "    if TU not in target_TUs:\n",
        "      target_TUs.add(TU)\n",
        "\n",
        "for test_input_text in test_input_texts:\n",
        "  for TU in test_input_text.split():\n",
        "    if TU not in input_TUs:\n",
        "      input_TUs.add(TU)\n",
        "\n",
        "input_TUs = sorted(list(input_TUs))\n",
        "target_TUs = sorted(list(target_TUs))\n",
        "num_encoder_tokens = len(input_TUs)\n",
        "num_decoder_tokens = len(target_TUs)\n",
        "max_encoder_seq_length = 25\n",
        "max_decoder_seq_length = 25\n",
        "\n",
        "input_token_index = dict([(TU,i) for i, TU in enumerate(input_TUs)])\n",
        "target_token_index = dict([(TU,i) for i, TU in enumerate(target_TUs)])\n",
        "\n",
        "input_TUs"
      ],
      "metadata": {
        "id": "bhY8QSE08RQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Input and Target Data"
      ],
      "metadata": {
        "id": "MOvPyzEp9uOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = np.zeros(\n",
        "(len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "\n",
        "decoder_input_data = np.zeros(\n",
        "(len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "decoder_target_data = np.zeros(\n",
        "(len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "#test_encoder _input_data = np.zeros(\n",
        "#(len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")"
      ],
      "metadata": {
        "id": "oTCncwAn9vHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder decoder data setup"
      ],
      "metadata": {
        "id": "bZLxGBeS_HHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (input_text, target_text, test_input_text) in enumerate(zip(input_texts, target_texts, test_input_texts)):\n",
        "  for t, TU in enumerate(input_text.split()):\n",
        "    encoder_input_data[i, t, input_token_index [TU]] = 1.0\n",
        "  for t, TU in enumerate(target_text.split()):\n",
        "    #decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "    decoder_input_data[i, t, target_token_index [TU]] = 1.0\n",
        "    if t > 0:\n",
        "      #decoder_target_data will be ahead by one timestep\n",
        "      # and will not include the start character.\n",
        "      decoder_target_data[i, t - 1, target_token_index [TU]] = 1.0\n",
        "\n",
        "  decoder_input_data[i, t + 1 :, target_token_index [\"$\"]] = 1.0\n",
        "  decoder_target_data[i, t:, target_token_index [\"$\"]] = 1.0\n",
        "\n",
        "for (i, test_input_text) in enumerate(test_input_texts):\n",
        "  for t, TU in enumerate(test_input_text.split()):\n",
        "    test_encoder_input_data[i, t, input_token_index [TU]] = 1.0"
      ],
      "metadata": {
        "id": "Nw44S3bW_JoR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "eb31678b-0d31-4cb9-b3fd-a116e820578b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5eb8971f231e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTU\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_token_index\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTU\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTU\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#decoder_target_data is ahead of decoder_input_data by one timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_texts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Setup"
      ],
      "metadata": {
        "id": "I2fH6hVc_9CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = keras. Input (shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.Bidirectional(keras.layers.LSTM(latent_dim,return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder (encoder_inputs)\n",
        "state_h= Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = keras. Input (shape=(None, num_decoder_tokens))\n",
        "\n",
        "decoder_lstm = keras.layers.LSTM( latent_dim*2, return_sequences=True, return_state=True)\n",
        "decoder_outputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "Tx-L2ot5AAF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training"
      ],
      "metadata": {
        "id": "0j2T_wt0A5oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=checkpoint_filepath,\n",
        "  save_weights_only=False,\n",
        "  monitor='val_loss',\n",
        "  mode='min',\n",
        "  save_best_only=True)\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping (patience=5, restore_best_weights=True)\n",
        "\n",
        "training_limit = num_samples - test_count\n",
        "#Adam (learning_rate=3e-4)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=3e-4), loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "  [encoder_input_data[:training_limit], decoder_input_data[:training_limit]],\n",
        "  decoder_target_data[:training_limit],\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[model_checkpoint_callback, early_stop_callback]\n",
        ")\n",
        "model.save(model_id)"
      ],
      "metadata": {
        "id": "E9EXmdFQA6Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Model"
      ],
      "metadata": {
        "id": "CRRkrjFcB4lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(model_id)\n",
        "\n",
        "encoder_inputs = model.input[0] # input_1\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = model.layers [1].output # lstm_1\n",
        "state_h_enc = Concatenate()([forward_h, backward_h])\n",
        "state_c_enc = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1] # input_2\n",
        "decoder_state_input_h= keras.Input (shape=(latent_dim*2,))\n",
        "decoder_state_input_c = keras. Input (shape=(latent_dim*2,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers [5]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "  decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers [6]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model= keras.Model(\n",
        "[decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")"
      ],
      "metadata": {
        "id": "gplHJ3UAB5ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoding"
      ],
      "metadata": {
        "id": "-bqgoNz3CspB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "reverse_index_df = pd.DataFrame (reverse_target_char_index.items())\n",
        "reverse_index_df.to_csv (r' reverse_index_df.csv', index = False, header=False)\n",
        "\n",
        "input_token_index_df = pd.DataFrame.from_dict(input_token_index.items())\n",
        "input_token_index_df.to_csv (r'input_token_index.csv', index = False, header=False)\n",
        "\n",
        "target_token_index_df = pd.DataFrame.from_dict(target_token_index.items())\n",
        "target_token_index_df.to_csv (r'target_token_index.csv', index = False, header=False)\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  target_seq= np.zeros((1, 1, num_decoder_tokens))\n",
        "\n",
        "  target_seq[0, 0, target_token_index [\"<S>\"]] = 1.0\n",
        "\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = \"\"\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict( [target_seq] + states_value)\n",
        "  \n",
        "    sampled_token_index = np.argmax (output_tokens [0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "    if sampled_char == \"<E>\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "      stop_condition = True\n",
        "    else:\n",
        "      decoded_sentence += sampled_char\n",
        "      decoded_sentence += \"\"\n",
        "\n",
        "    target_seq= np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "    states_value = [h, c]\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "YaZAb6lGCvMR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}