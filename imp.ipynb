{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/randiepathirage/4th/blob/main/imp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrztVpS_pBTF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from unicodedata import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM,Bidirectional,Input,Concatenate\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaH37qy0qczw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37a778a-42e5-4941-9227-0d7b0ae33e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "nwNfiQy2Moi2",
        "outputId": "4dec6f7f-8365-4ce4-fc7b-49b76a7608b9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-400dac0d8fe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"singlish\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sinhala\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/LTRL_Parallel_Words_3_Shuffled.csv'"
          ]
        }
      ],
      "source": [
        "# DATA_PATH = '/content/drive/MyDrive/UNI/4th Year/Research/DataSet/LTRL_Clean.tsv'\n",
        "#DATA_PATH = '/content/drive/MyDrive/4th year Research/data collection/data/LTRL_Parallel_Words_3_Shuffled.csv'\n",
        "DATA_PATH = '/content/LTRL_Parallel_Words_3_Shuffled.csv'\n",
        "\n",
        "\n",
        "model_id = \"e5_1\"\n",
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 50 # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 24415  # Number of samples to train on.\n",
        "test_count = 5000\n",
        "\n",
        "df = pd.read_csv(DATA_PATH, names=[\"singlish\",\"sinhala\"],dtype=str).astype(str)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input_data = pd.DataFrame(df.singlish.values[:num_samples])\n",
        "input_data.columns = ['singlish']\n",
        "\n",
        "target_data = pd.DataFrame(df.sinhala.values[:num_samples])\n",
        "target_data.columns = ['sinhala']\n",
        "\n",
        "#@title Preview dataset\n",
        "preview_dataset = True #@param {type:\"boolean\"}\n",
        "\n",
        "if(preview_dataset):\n",
        "  print(\"Singlish\")\n",
        "  print(input_data.head())\n",
        "  print(\"Sinhala\")\n",
        "  print(target_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2_HmDFYNJRW"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "  text = normalize('NFD', text).encode('utf-8')\n",
        "  return text.decode('UTF-8')\n",
        "\n",
        "def clean_and_normalize(text):\n",
        "  text = normalize_text(text.lower())\n",
        "  return text\n",
        "\n",
        "def print_sentence_length_status(seqeunce):\n",
        "  lt10 = lt20 = lt30 = lt40 = lt50 = lg50 = 0\n",
        "  for line in seqeunce:\n",
        "    c = len(line.split())\n",
        "    if(c<=10):\n",
        "      lt10+=1\n",
        "    elif(c<=20):\n",
        "      lt20+=1\n",
        "    elif(c<=30):\n",
        "      lt30+=1\n",
        "    elif(c<=40):\n",
        "      lt40+=1\n",
        "    elif(c<=50):\n",
        "      lt50+=1\n",
        "    else:\n",
        "      lg50+=1\n",
        " \n",
        "  print(\"LOE10 : \",lt10)\n",
        "  print(\"LOE20 : \",lt20)\n",
        "  print(\"LOE30 : \",lt30)\n",
        "  print(\"LOE40 : \",lt40)\n",
        "  print(\"LOE50 : \",lt50)\n",
        "  print(\"GTE50  : \",lg50)\n",
        "  print(\"Total sentence count : \", lt10+lt20+lt30+lt40+lt50+lg50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA7bhzc1NOp_"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "print_stat = True #@param {type:\"boolean\"}\n",
        "if(print_stat):\n",
        "  print_sentence_length_status(input_data['singlish'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7R524wwNfag"
      },
      "outputs": [],
      "source": [
        "def removeUnnecessaryRepeats(word):\n",
        "  prev = word[0]\n",
        "  repeatCount = 1\n",
        "  ans = word[0]\n",
        "  for i in range(1,len(word)):\n",
        "    if(word[i]==prev):\n",
        "      repeatCount+=1\n",
        "    else:\n",
        "      repeatCount=1\n",
        "    \n",
        "    if(repeatCount<3):\n",
        "      ans+=word[i]\n",
        "      prev = word[i]\n",
        "\n",
        "  return ans\n",
        "\n",
        "def preProcessSinglish(singText):\n",
        "  vowels =  ['a','e','i','o','u']\n",
        "  specialConsonentSuffix = ['h']\n",
        "  singText = singText.lower()\n",
        "  singWords = singText.split()\n",
        "  finList = []\n",
        "  for singWord in singWords:\n",
        "    singWord = removeUnnecessaryRepeats(singWord)\n",
        "    buf = \"\"\n",
        "    for i in range(len(singWord)):\n",
        "      char = singWord[i]\n",
        "\n",
        "      if(char in vowels):\n",
        "        if(i<len(singWord)-1):\n",
        "          if(char == singWord[i+1]):\n",
        "            buf+=char\n",
        "          else:\n",
        "            buf+=char\n",
        "            finList.append(buf)\n",
        "            buf=\"\"\n",
        "        else:\n",
        "          buf+=char\n",
        "          finList.append(buf)\n",
        "          buf=\"\"\n",
        "      else:\n",
        "        if(i<len(singWord)-1):\n",
        "          if(char == singWord[i+1] or (singWord[i+1] not in vowels and singWord[i+1] not in specialConsonentSuffix)):\n",
        "            buf+=char\n",
        "            finList.append(buf)\n",
        "            buf=\"\"\n",
        "          else:\n",
        "            buf+=char\n",
        "        else:\n",
        "          buf+=char\n",
        "          finList.append(buf)\n",
        "          buf=\"\"\n",
        "    finList.append(\"$\")\n",
        "\n",
        "  return \" \".join(finList)\n",
        "\n",
        "def preProcessSinhala(sinText):\n",
        "  sinWords = sinText.split()\n",
        "  finList = []\n",
        "  for sinWord in sinWords:\n",
        "    charList = re.findall(r'\\w\\W?',sinWord.strip())\n",
        "    finList.extend(charList)\n",
        "    finList.append(\"$\")\n",
        "  return \" \".join(finList)\n",
        "\n",
        "input_data['singlish'] = input_data['singlish'].apply(lambda x: preProcessSinglish(clean_and_normalize(x)))\n",
        "target_data['sinhala'] = target_data['sinhala'].apply(lambda x: preProcessSinhala(clean_and_normalize(x))).apply(lambda x: '<S> '+x+' <E>')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp61wF2HNmhj"
      },
      "outputs": [],
      "source": [
        "#@title View Dataset Heads\n",
        "view_dataset_head = True #@param {type:\"boolean\"}\n",
        "if(view_dataset_head):\n",
        "  print(input_data['singlish'].head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhE0ENx2gDig"
      },
      "outputs": [],
      "source": [
        "max_encoder_length = 250\n",
        "max_decoder_length = 250\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_TUs = set()\n",
        "target_TUs = set()\n",
        "\n",
        "input_texts = input_data.singlish\n",
        "target_texts = target_data.sinhala\n",
        "\n",
        "for input_text in input_texts:\n",
        "  for TU in input_text.split():\n",
        "      if TU not in input_TUs:\n",
        "          input_TUs.add(TU)\n",
        "  \n",
        "for target_text in target_texts:\n",
        "  for TU in target_text.split():\n",
        "      if TU not in target_TUs:\n",
        "          target_TUs.add(TU)\n",
        "\n",
        "input_TUs = sorted(list(input_TUs))\n",
        "target_TUs = sorted(list(target_TUs))\n",
        "num_encoder_tokens = len(input_TUs)\n",
        "num_decoder_tokens = len(target_TUs)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(TU, i) for i, TU in enumerate(input_TUs)])\n",
        "target_token_index = dict([(TU, i) for i, TU in enumerate(target_TUs)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts"
      ],
      "metadata": {
        "id": "wuUIvlZfw1My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j2oI6MTgHhT"
      },
      "outputs": [],
      "source": [
        "target_token_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsG25U3ugUHK"
      },
      "outputs": [],
      "source": [
        "input_token_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLhzFZrG426i"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, TU in enumerate(input_text.split()):\n",
        "        encoder_input_data[i, t, input_token_index[TU]] = 1.0\n",
        "    for t, TU in enumerate(target_text.split()):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[TU]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[TU]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\"$\"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\"$\"]] = 1.0\n",
        "\n",
        "target_texts[0].split()\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.Bidirectional(keras.layers.LSTM(latent_dim, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "print(state_h.shape)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "training_limit = num_samples-test_count\n",
        "# Adam(learning_rate=3e-4)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=3e-4), loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"], \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5zh0xlq5Lu6"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCUL9dpogdo5"
      },
      "outputs": [],
      "source": [
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTwbL_62ggQF"
      },
      "outputs": [],
      "source": [
        "reverse_target_char_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lZaYR5zgo_X"
      },
      "outputs": [],
      "source": [
        "target_token_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np0zkTeXH94A"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    [encoder_input_data[:training_limit], decoder_input_data[:training_limit]],\n",
        "    decoder_target_data[:training_limit],\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[model_checkpoint_callback]\n",
        ")\n",
        "\n",
        "plt.plot(history.history['loss'], label='Loss (training data)')\n",
        "plt.plot(history.history['val_loss'], label='Loss (validation data)')\n",
        "plt.title('Singlish to Sinhala Transliteration')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('model_plot.png')\n",
        "\n",
        "model.save(model_id)\n",
        "\n",
        "target_token_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8I_tVMT4f7c"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(model_id)\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = model.layers[1].output  # lstm_1\n",
        "state_h_enc = Concatenate()([forward_h, backward_h])\n",
        "state_c_enc = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim*2,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[5]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[6]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    \n",
        "    target_seq[0, 0, target_token_index[\"<S>\"]] = 1.0\n",
        "\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "        if sampled_char == \"<E>\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "          stop_condition = True\n",
        "        else:\n",
        "          decoded_sentence += sampled_char\n",
        "          decoded_sentence += \" \"\n",
        "\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "reverse_index_df = pd.DataFrame(reverse_target_char_index.items())\n",
        "reverse_index_df.to_csv (r'reverse_index_df.csv', index = False, header=False)\n",
        "\n",
        "input_token_index_df = pd.DataFrame.from_dict(input_token_index.items())\n",
        "input_token_index_df.to_csv (r'input_token_index.csv', index = False, header=False)\n",
        "\n",
        "target_token_index_df = pd.DataFrame.from_dict(target_token_index.items())\n",
        "target_token_index_df.to_csv (r'target_token_index.csv', index = False, header=False)\n",
        "\n",
        "unseen_file = open(\"unseen_data.txt\", \"w\")\n",
        "unseen_data_target = []\n",
        "unseen_data_candidate = []\n",
        "i = 0\n",
        "for seq_index in range(num_samples-test_count,num_samples):\n",
        "    i+=1\n",
        "    #print(\"unseen data \",i)\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    # print(input_seq.shape)\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    unseen_file.write(\"-\\n\")\n",
        "    input_sentence = input_texts[seq_index]\n",
        "    target_sentence = target_texts[seq_index]\n",
        "    target_sentence = target_sentence.replace(\"<S>\", \"\")\n",
        "    target_sentence = target_sentence.replace(\"<E>\", \"\")\n",
        "\n",
        "    unseen_data_target.append(target_sentence)\n",
        "    unseen_data_candidate.append(decoded_sentence)\n",
        "\n",
        "    input_sentence = input_sentence.replace(\" \",\"\")\n",
        "    input_sentence = input_sentence.replace(\"$\",\" \")\n",
        "\n",
        "    unseen_file.write(input_sentence+\"\\n\")\n",
        "    unseen_file.write(target_sentence + \"\\n\")\n",
        "    unseen_file.write(decoded_sentence+\"\\n\")\n",
        "\n",
        "df_unseen_target = pd.DataFrame(unseen_data_target)\n",
        "df_unseen_candidate = pd.DataFrame(unseen_data_candidate)\n",
        "df_unseen_target.to_csv (r'unseen_target.csv', index = False, header=False)\n",
        "df_unseen_candidate.to_csv (r'unseen_candidate.csv', index = False, header=False)\n",
        "unseen_file.close()\n",
        "\n",
        "seen_file = open(\"seen_data.txt\", \"w\")\n",
        "seen_data_target = []\n",
        "seen_data_candidate = []\n",
        "i=0\n",
        "for seq_index in range(min(2000,num_samples - test_count)):\n",
        "    i+=1\n",
        "    print(\"seen data \",i)\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    # print(input_seq)\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    input_sentence = input_texts[seq_index]\n",
        "    target_sentence = target_texts[seq_index]\n",
        "    target_sentence = target_sentence.replace(\"<S>\", \"\")\n",
        "    target_sentence = target_sentence.replace(\"<E>\", \"\")\n",
        "\n",
        "    seen_data_target.append(target_sentence)\n",
        "    seen_data_candidate.append(decoded_sentence)\n",
        "\n",
        "    input_sentence = input_sentence.replace(\" \",\"\")\n",
        "    input_sentence = input_sentence.replace(\"$\",\" \")\n",
        "\n",
        "    seen_file.write(\"-\\n\")\n",
        "    seen_file.write(input_sentence + \"\\n\")\n",
        "    seen_file.write(target_sentence + \"\\n\")\n",
        "    seen_file.write(decoded_sentence + \"\\n\")\n",
        "\n",
        "df_seen_target = pd.DataFrame(seen_data_target)\n",
        "df_seen_candidate = pd.DataFrame(seen_data_candidate)\n",
        "df_seen_target.to_csv (r'seen_target.csv', index = False, header=False)\n",
        "df_seen_candidate.to_csv (r'seen_candidate.csv', index = False, header=False)\n",
        "seen_file.close()\n",
        "\n",
        "def process_text(text):\n",
        "  # breaak into words\n",
        "  words = text.split()\n",
        "  for word in words:\n",
        "    # convert to TUs\n",
        "    preprocessed_word = preProcessSinglish(clean_and_normalize(word))\n",
        "    # create one hot embedding\n",
        "    encoded_input = np.zeros(\n",
        "        (1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    for t, TU in enumerate(input_text.split()):\n",
        "        encoded_input[0, t, input_token_index[TU]] = 1.0\n",
        "    encoded_input[0, t + 1 :, input_token_index[\"$\"]] = 1.0\n",
        "    # predict\n",
        "    decoded_sentence = decode_sequence(encoded_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LygZeUUY8HyC"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "score = corpus_bleu(unseen_data_target, unseen_data_candidate)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKZtAWUopMSV"
      },
      "outputs": [],
      "source": [
        "def removeUnnecessaryRepeats(word):\n",
        "  prev = word[0]\n",
        "  repeatCount = 1\n",
        "  ans = word[0]\n",
        "  for i in range(1,len(word)):\n",
        "    if(word[i]==prev):\n",
        "      repeatCount+=1\n",
        "    else:\n",
        "      repeatCount=1\n",
        "    \n",
        "    if(repeatCount<3):\n",
        "      ans+=word[i]\n",
        "      prev = word[i]\n",
        "\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLIYck19pF5T"
      },
      "outputs": [],
      "source": [
        "def preProcessSinglish(singText):\n",
        "  vowels =  ['a','e','i','o','u']\n",
        "  specialConsonentSuffix = ['h']\n",
        "  singText = singText.lower()\n",
        "  singWords = singText.split()\n",
        "  finList = []\n",
        "  for singWord in singWords:\n",
        "    singWord = removeUnnecessaryRepeats(singWord)\n",
        "    buf = \"\"\n",
        "    for i in range(len(singWord)):\n",
        "      char = singWord[i]\n",
        "\n",
        "      if(char in vowels):\n",
        "        if(i<len(singWord)-1):\n",
        "          if(char == singWord[i+1]):\n",
        "            buf+=char\n",
        "          else:\n",
        "            buf+=char\n",
        "            finList.append(buf)\n",
        "            buf=\"\"\n",
        "        else:\n",
        "          buf+=char\n",
        "          finList.append(buf)\n",
        "          buf=\"\"\n",
        "      else:\n",
        "        if(i<len(singWord)-1):\n",
        "          if(char == singWord[i+1] or (singWord[i+1] not in vowels and singWord[i+1] not in specialConsonentSuffix)):\n",
        "            buf+=char\n",
        "            finList.append(buf)\n",
        "            buf=\"\"\n",
        "          else:\n",
        "            buf+=char\n",
        "        else:\n",
        "          buf+=char\n",
        "          finList.append(buf)\n",
        "          buf=\"\"\n",
        "    finList.append(\"$\")\n",
        "\n",
        "  return \" \".join(finList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "somM1KYvpOek"
      },
      "outputs": [],
      "source": [
        "preProcessSinglish(\"aakaaarura\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}